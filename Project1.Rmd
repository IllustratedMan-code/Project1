---
title: "Project1"
author: David Lewis
output: word_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)

# FIXME: change the input file to TCGA_breast_cancer_ERpositive_vs_ERnegative_PAM50.tsv

# file <- "TCGA_breast_cancer_LumA_vs_Basal_PAM50.tsv"
file <- "TCGA_breast_cancer_ERpositive_vs_ERnegative_PAM50.tsv"
first10 <- c("NAT1", "BIRC5", "BAG1", "BCL2", "BLVRA", "CCNB1", "CCNE1", "CDC6", "CDC20", "CDH3")
```

## Assignment
1.	for the assignment use the second dataset called TCGA_breast_cancer_ERpositive_vs_ERnegative_PAM50.tsv that shows ER assignment for each sample (Positive vs. Negative)
2.	compute 5-fold and 10-fold cross-validation estimates of prediction accuracies of ER using all genes by utilizing logistic regression and compare with NNC (2x2 table).
3.	modify the the R markdown document template to report your computation and results in a table format.
4.	comment on the quality of results
5.	In the second part of the assignment use Project1fs.R to process a large data set by first removing all genes with sd < 1 and subsequently use Feature selection to pick top 50 genes vs top 100 genes for cross-validation based on the t-test statistic.
6.	For extra credit â€“ please replace centroid based classifier with one utilizing logistic or lasso regression similarly to the first part of the assignment and report on any difficulties.

## Reading data

Please add R code that reads data here - 
reading file: `r file`


```{r reading_data, echo=FALSE}
system.time({
  # important -- this makes sure our runs are consistent and reproducible
  set.seed(0)

  header <- scan(file, nlines = 1, sep = "\t", what = character())
  data <- read.table(file, skip = 2, header = FALSE, sep = "\t", quote = "", check.names = FALSE)

  header[1] <- "gene_id"
  names(data) <- header

  header2 <- scan(file, skip = 1, nlines = 1, sep = "\t", what = character())
})
```

## Computation

Please add R code that computes the results

```{r computation, echo=FALSE}
cross_validation <- function(nfold, alg = "centroid") {
  # split each cancer type samples into nfold groups
  LumA_groups <- split(sample(colnames(LumA)), 1 + (seq_along(colnames(LumA)) %% nfold))
  Basal_groups <- split(sample(colnames(Basal)), 1 + (seq_along(colnames(Basal)) %% nfold))

  result <- array()

  # iterate from 1 to nfold groups -- to choose test group
  for (test_group in 1:nfold) {
    # return all samples in the chosen test group
    testLumA <- LumA[, colnames(LumA) %in% unlist(LumA_groups[test_group])]
    testBasal <- Basal[, colnames(Basal) %in% unlist(Basal_groups[test_group])]

    # return all samples *not* in the chosen test group
    trainingLumA <- LumA[, !(colnames(LumA) %in% unlist(LumA_groups[test_group]))]
    trainingBasal <- Basal[, !(colnames(Basal) %in% unlist(Basal_groups[test_group]))]

    if (alg == "centroid") {
      # compute centroid for each cancer type -- mean for each gene based on all samples
      # note -- rows are gene
      centroidLumA <- rowMeans(trainingLumA)
      centroidBasal <- rowMeans(trainingBasal)

      # For each sample in the test set decide whether it will be classified
      # distance from centroid Lum A: sum(abs(x-centroidLumA))
      # distance from centroid Basal: sum(abs(x-centroidBasal))
      # distance is a sum of distances over all genes
      # misclassification if when the distance is greater from centroid associated with known result
      misclassifiedLumA <<- sum(sapply(testLumA, function(x) {
        sum(abs(x - centroidLumA)) > sum(abs(x - centroidBasal))
      }))
      misclassifiedBasal <<- sum(sapply(testBasal, function(x) {
        sum(abs(x - centroidLumA)) < sum(abs(x - centroidBasal))
      }))
    }

    if (alg == "GLM") {
      # FIXME: ADD GLM
      trainingAB <- rbind(
        cbind(data.frame(t(trainingLumA)), cancer = 0),
        cbind(data.frame(t(trainingBasal)), cancer = 1)
      )
      testLumA0 <- data.frame(t(testLumA))
      testBasal0 <- data.frame(t(testBasal))
      model <- glm(cancer ~ ., trainingAB, family = "binomial")
      p <- predict(model, newdata = testLumA0, type = "response")
      misclassifiedLumA <<- sum(ifelse(p < 0.5, 0, 1))
      p <- predict(model, newdata = testBasal0, type = "response")
      misclassifiedBasal <<- sum(ifelse(p >= 0.5, 0, 1))
    }

    result[test_group] <- (misclassifiedLumA + misclassifiedBasal) / (ncol(testLumA) + ncol(testBasal))
  }

  # c(mean(result), sd(result))
  paste("mean=", round(mean(result), 4), "sd=", round(sd(result), 4))
}


system.time({
  # FIXME: the data file has ER "Positive" / "Negative" instead of cancer type
  # FIXED

  LumA <- data[, header2 == "Positive"]
  Basal <- data[, header2 == "Negative"]

  kNNC_5_all <- cross_validation(nfold = 5)
  GLM_5_all <- cross_validation(nfold = 5, alg = "GLM")

  # FIXME: add code to compute 10 fold cross validation for GLM and kNNC
  # FIXED
  kNNC_10_all <- cross_validation(nfold = 10)
  GLM_10_all <- cross_validation(nfold = 10, alg = "GLM")
})
```



Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code.

## Results

These are our results:

### 5-fold cross validation
```{r results5, echo=FALSE}
x <- data.frame("GLM" = c(GLM_5_all, GLM_10_all), "kNNC" = c(kNNC_5_all, kNNC_10_all))
rownames(x) <- c("5-fold", "10-fold")
kable(x)
```


## Discussion

For five-fold and 10-fold validation, the GLM model has slightly worse performance according to the mean when compared to the kNNC method. 
However, the sd goes up when the number of folds increases in kNNC while it decreases in the GLM model. It could be argued that the performance of the 
two models is too similar to argue for the use of one or the other.

# Part 2

Change eval=TRUE when ready to include Project1fs.R

```{r part2, eval = TRUE, code=readLines("Project1fs.R"), echo=FALSE}

```
